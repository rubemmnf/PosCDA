{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_CNN_TextClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGPx2I9UyB0t",
        "outputId": "6acd838e-d230-4acd-f9fd-82f4ec414857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/valmirf/mineracao_textual.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mineracao_textual'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 148 (delta 74), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (148/148), 4.14 MiB | 4.68 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCeIJRCGM4s"
      },
      "source": [
        "# Classificação de Textos com CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFFknubSGeC_"
      },
      "source": [
        "## Carregar Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPMfiTT9y4p",
        "outputId": "99c1c244-8de7-4a34-9706-8c74b92641a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = data = pd.read_csv(\"mineracao_textual/Dados/ClothingReviews.csv\")\n",
        "\n",
        "# descartar classes minoritarias\n",
        "major_classes = list(df['Class Name'].value_counts()[0:4].index)\n",
        "majority      = df.loc[df['Class Name'].isin(major_classes),]\n",
        "minority      = df.loc[~df['Class Name'].isin(major_classes),]\n",
        "\n",
        "print('\\n',5*'=', 'major classes: \\n',    majority['Class Name'].value_counts()[0:4])\n",
        "print('\\n',5*'=', 'minority classes: \\n', minority['Class Name'].value_counts()[4:])\n",
        "\n",
        "majority.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ===== major classes: \n",
            " Dresses     96\n",
            "Knits       61\n",
            "Blouses     54\n",
            "Sweaters    20\n",
            "Name: Class Name, dtype: int64\n",
            "\n",
            " ===== minority classes: \n",
            " Fine gauge    11\n",
            "Outerwear      6\n",
            "Sleep          5\n",
            "Lounge         5\n",
            "Intimates      4\n",
            "Legwear        4\n",
            "Swim           4\n",
            "Layering       3\n",
            "Shorts         3\n",
            "Tops           1\n",
            "Name: Class Name, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Clothing ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Title</th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Recommended IND</th>\n",
              "      <th>Positive Feedback Count</th>\n",
              "      <th>Division Name</th>\n",
              "      <th>Department Name</th>\n",
              "      <th>Class Name</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13992</td>\n",
              "      <td>862</td>\n",
              "      <td>38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A comfortable shirt that has a touch of dressy...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>General</td>\n",
              "      <td>Tops</td>\n",
              "      <td>Knits</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2603</td>\n",
              "      <td>863</td>\n",
              "      <td>64</td>\n",
              "      <td>Very cute</td>\n",
              "      <td>A couple of months ago i bought the white in t...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>General</td>\n",
              "      <td>Tops</td>\n",
              "      <td>Knits</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4339</td>\n",
              "      <td>1094</td>\n",
              "      <td>29</td>\n",
              "      <td>Runs very small</td>\n",
              "      <td>A cute dress, though waist a little too high (...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12870</td>\n",
              "      <td>1078</td>\n",
              "      <td>48</td>\n",
              "      <td>Wish it had a little more shape</td>\n",
              "      <td>A cute dress. i did not have the issue with th...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>General Petite</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1745</td>\n",
              "      <td>888</td>\n",
              "      <td>54</td>\n",
              "      <td>Nice top</td>\n",
              "      <td>A cute top. slightly tight in the upper area. ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>General</td>\n",
              "      <td>Tops</td>\n",
              "      <td>Knits</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Clothing ID  Age  ... Department Name Class Name Unnamed: 11\n",
              "0       13992          862   38  ...            Tops      Knits         NaN\n",
              "1        2603          863   64  ...            Tops      Knits         NaN\n",
              "3        4339         1094   29  ...         Dresses    Dresses         NaN\n",
              "4       12870         1078   48  ...         Dresses    Dresses         NaN\n",
              "5        1745          888   54  ...            Tops      Knits         NaN\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZ4Q5LSGmLi"
      },
      "source": [
        "## Codificar Labels e Separar treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZFGpNA3-ppf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "_labels = majority['Class Name'].values.reshape((len(majority['Class Name']), 1))\n",
        "X = majority['Review Text']\n",
        "y = encoder.fit_transform(_labels)\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test  = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDQrrr-V_XqJ"
      },
      "source": [
        "sentences_train[:4], y_train[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNp-bp4REldT"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86mnIfM4Gw8o"
      },
      "source": [
        "## Tokenizando Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-JGH1BV-bzh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train   = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test    = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "\n",
        "print(vocab_size)\n",
        "print(sentences_train.iloc[2])\n",
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgf1QWbGV0A7"
      },
      "source": [
        "#X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmbodUTYEu5w"
      },
      "source": [
        "#list(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxD8N8AeG2bF"
      },
      "source": [
        "## Sequence Padding\n",
        "\n",
        "Um problema que temos é que cada sequência de texto tem na maioria dos casos diferentes comprimentos de palavras. Para corrigir isso, você pode usar `pad_sequence()` que simplesmente preenche a sequência de palavras com zeros. \n",
        "\n",
        "Por padrão, ele anexa zeros, mas queremos anexá-los. Normalmente, não importa se você acrescenta ou acrescenta zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McfGfFYo_CUB"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train[0, :], y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TFkefspHOeB"
      },
      "source": [
        "## Arquitetura da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMnG0lEAvLr"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpY9AnPhA07I"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koIALEmzHabN"
      },
      "source": [
        "## Adicionando embeddings pré-treinados\n",
        "É possível usarmos embeddings pré-treinados. A escolha é sempre relativa ao seu problema. Por exemplo, se você precisa resolver um problema de classificação de texto de cunho geral, pode pegar um Embeddings pré-treinado do Google, com milhões de textos. Porém, se quiser resolver um problema de classificação de sentimentos de review de livros, pode ser útil utilizar um embeddings mais próximo do seu problema, como por exemplo, um embeddings pré-treinado com informações e review de livros da Amazon. Nessa atividade, vamos utilizar....\n",
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzOVZ2vzCXQC"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-D4hRM2CV7y"
      },
      "source": [
        "!head -n 1 googlenews.word2vec.300d.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRPhGLTDHo8r"
      },
      "source": [
        "### GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eqJimlgHrAe"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHiLHMT6HuKF"
      },
      "source": [
        "!head -n 1 glove.840B.300d.sst.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyXEVU1H3Ka"
      },
      "source": [
        "## Teste: Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkMbEWw6A34u"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "    \n",
        "embedding_matrix = create_embedding_matrix('googlenews.word2vec.300d.txt',\n",
        "                      tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYtX9OjZDLXj"
      },
      "source": [
        "embedding_matrix[0:3,:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psvpHf34DRwL"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ2p3j8jDaUY"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7bDmmx5H9Ft"
      },
      "source": [
        "## Teste: Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41qSXd0sDc7w"
      },
      "source": [
        "embedding_matrix = create_embedding_matrix('glove.840B.300d.sst.txt',\n",
        "                      tokenizer.word_index, embedding_dim)\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2HNzu7fF-Eq"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2S9rGT-JRIW"
      },
      "source": [
        "## Métricas detalhadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KADwY35dINUf"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "preds = model.predict_classes(X_test)\n",
        "true  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(metrics.classification_report(true, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3YTYJ5Jfnd"
      },
      "source": [
        "## Como melhorar?\n",
        "\n",
        "- Hyperparameter tuning\n",
        "- Adicionar mais filtros (convolution layers), por exemplo:\n",
        "\n",
        "```\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "\n",
        "...\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "\n",
        "...\n",
        "```"
      ]
    }
  ]
}