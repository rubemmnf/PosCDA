{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03b-AssociacaoPalavras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19l98qLhsjIQ"
      },
      "source": [
        "# Associação de palavras com NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq_2H_ZbsaCn"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt') # Punkt sentence tokenizer\n",
        "from nltk.probability import FreqDist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mqH0JnLs36z"
      },
      "source": [
        "# Tokenização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR6qC_KwsuDi"
      },
      "source": [
        "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n",
        "tokenized_text=sent_tokenize(text)\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHbUwyahs6id"
      },
      "source": [
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fyjGn-ps_9A"
      },
      "source": [
        "# Distribuição de frequência "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q57-Rw8es9ou"
      },
      "source": [
        "fdist = FreqDist(tokenized_word)\n",
        "print(fdist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzj2P8sPtFnJ"
      },
      "source": [
        "fdist.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHjHqEhItHQp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g98A8k07tQSQ"
      },
      "source": [
        "## Remover stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cALpkBA4tI5f"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peXlA2z_tTho"
      },
      "source": [
        "filtered_sent=[]\n",
        "for w in tokenized_word:\n",
        "    if w not in stop_words:\n",
        "        filtered_sent.append(w)\n",
        "print(\"Tokenized Sentence:\",tokenized_word)\n",
        "print(\"Filterd Sentence:\",filtered_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UawtuucatWeO"
      },
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_words=[]\n",
        "for w in filtered_sent:\n",
        "    stemmed_words.append(ps.stem(w))\n",
        "\n",
        "print(\"Filtered Sentence:\",filtered_sent)\n",
        "print(\"Stemmed Sentence:\",stemmed_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFCLgs3KtaMD"
      },
      "source": [
        "fdist_stemmed = FreqDist(stemmed_words)\n",
        "fdist_stemmed.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKBayQhDtprZ"
      },
      "source": [
        "# remove punkt\n",
        "filtered_sent=[]\n",
        "stemmed_words=[]\n",
        "for w in filtered_sent:\n",
        "    stemmed_words.append(ps.stem(w))\n",
        "for w in tokenized_word:\n",
        "    if w not in stop_words and w.isalnum():\n",
        "      stemmed_words.append(ps.stem(w))\n",
        "fdist_stemmed = FreqDist(stemmed_words)\n",
        "fdist_stemmed.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvjMmNxZtcdf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqHzMooAu8bj"
      },
      "source": [
        "# n-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI-YwjUVtjDQ"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
        "\n",
        "n = 2\n",
        "sixgrams = ngrams(sentence.split(), n)\n",
        "\n",
        "for grams in sixgrams:\n",
        "  print (grams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXfSKWWVvD20"
      },
      "source": [
        "# WordNet\n",
        "\n",
        "Cada token pode ser associado a um **synset**. Um **synset** é identificado com um nome de três partes do formulário, como \"word.pos.nn\", em que 'pos (part of speech)' é uma classe gramatical específica de palavras como substantivo, adjetivo ou verbo e 'nn' é um índice porque uma palavra pode ter múltiplos significados. Você pode ver a lista de sinônimos em um formulário com três partes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c3tGO61u__D"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZDgXL4xwOwe"
      },
      "source": [
        "dog = wn.synsets('dog')\n",
        "print(dog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaHD4AfKwlEj"
      },
      "source": [
        "Para acessar o significado de cada synset, utilizar o método `definition()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f7x11aPwtDl"
      },
      "source": [
        "for syn in wn.synsets('dog'):\n",
        "  print(syn, syn.definition())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx6oeXHkyzHC"
      },
      "source": [
        "Fecho transitivo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQWxLSZYvGi_"
      },
      "source": [
        "dog = wn.synset('dog.n.01')\n",
        "cat = wn.synset('cat.n.01')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW64DXLQyzRF"
      },
      "source": [
        "hyper = lambda s: s.hypernyms()\n",
        "display(dog.hypernyms())\n",
        "display(list(dog.closure(hyper)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQHuqFjtx7j6"
      },
      "source": [
        "O WordNet possui diferentes formas de calcular similaridades entre termos (lista completa [aqui](https://www.nltk.org/howto/wordnet.html)). Por exemplo, o método `path_similarity` retorna um score indicando como são semelhantes os sentidos de duas palavras, com base no caminho mais curto que conecta os sentidos na taxonomia is-a (hypernym / hypnoym)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AalVyK5Cxc1X"
      },
      "source": [
        "print(dog.path_similarity(cat))\n",
        "print(dog.wup_similarity(cat)) #distância baseada na profundidade na taxonomia e ancestral mais comum"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}