{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05b_Sentiment_Analysis_Imdb_Rubem_Novellino.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLJozfTGU0Lj"
      },
      "source": [
        "##Análise de Sentimentos\n",
        "A análise de sentimento visa determinar a opinião ou sentimento das pessoas eem relçao a um produto, pessoa, evento, ou qualquer outro objeto de interesse. Por exemplo, um palestrante ou escritor em relação a um documento, interação ou evento. O sentimento é principalmente categorizado em categorias positivas, negativas e neutras. Por meio da análise de sentimento, podemos prever, por exemplo, a opinião e a atitude de um cliente sobre um produto com base em uma resenha que ele escreveu. Essa técnica é amplamente aplicada a coisas como revisões, pesquisas, documentos e muito mais.\n",
        "\n",
        "##Base de Dados IMDB\n",
        "O conjunto de dados de classificação de sentimento do IMDB (https://www.imdb.com/) consiste em 50000 resenhas de filmes de usuários do IMDB que são rotuladas como positivas (1) ou negativas (0). As 50000 resenhas são divididas em 25000 para treinamento e 25000 para teste. O conjunto de dados foi criado por pesquisadores da Universidade de Stanford e publicado em um artigo de 2011, onde alcançou 88.89% de precisão. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxphGHVdu6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2663d6-3c31-403b-a862-9e9dea27a72f"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# Load the data, keeping only 10,000 of the most frequently occuring words\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oem7f6Prdu6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d16b35-5825-4dce-8fb4-2c39ccc743a5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Since we restricted ourselves to the top 10000 frequent words, no word index should exceed 10000\n",
        "# we'll verify this below\n",
        "# Here is a list of maximum indexes in every review --- we search the maximum index in this list of max indexes\n",
        "print(type([max(sequence) for sequence in train_data]))\n",
        "\n",
        "# Find the maximum of all max indexes\n",
        "max([max(sequence) for sequence in train_data])\n",
        "\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "\n",
        "print(\"Categories:\", np.unique(labels))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz1pTRryYWtb"
      },
      "source": [
        "Você pode ver acima que o conjunto de dados é classificado em duas categorias, 0 ou 1, que representa o sentimento da avaliação do filme, negativo e positivo respectivamente. Todo o conjunto de dados contém 9.998 palavras únicas e o comprimento médio da revisão é de 234 palavras, com um desvio padrão de 173 palavras. \n",
        "\n",
        "##Decodificação da Avaliação\n",
        "Abaixo, vamos decodificar o rótulo no formato *One-Hot Code*.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX33fS3rLltV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414e523c-9fef-4328-ec58-bef659305cf4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(labels.reshape(-1, 1))\n",
        "print(y[0])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeqvDdtaT8J"
      },
      "source": [
        "##Preparação dos Dados\n",
        "\n",
        "Nos códigos abaixo iremos realizar a separação dos dados em treinamento e teste utilizando a função `train_test_split` do sklearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOx2UvKtaynK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ced36c-7e5c-4dc7-b5f2-85f77b2f095f"
      },
      "source": [
        "sentences_train, sentences_test, y_train, y_test  = train_test_split(data, y, test_size=0.3, random_state=42, stratify=y)\n",
        "print(sentences_train[:2], y_train[:2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[list([1, 193, 4, 1003, 1544, 592, 1648, 2, 7, 4, 4997, 1493, 11, 49, 5867, 39, 2, 2, 668, 1879, 2, 5203, 6485, 5, 9358, 11, 49, 2509, 2, 34, 167, 4748, 25, 28, 4, 132, 7521, 2493, 42, 4, 693, 3908, 7, 128, 573, 17, 4, 250, 39, 2, 11, 4, 2997, 2, 15, 132, 39, 2, 63, 47, 164, 8, 81, 19, 14, 422, 14, 127, 193, 273, 11, 3829, 75, 215, 202, 12, 15, 42, 2, 17, 442, 82, 5414, 8, 9, 40, 35, 445, 310, 7, 3960, 7375, 256, 133, 34, 524, 2, 19, 15, 1081, 2, 1826, 63, 91, 7, 178, 86, 877, 2, 19, 11, 2, 50, 26, 1942, 491, 56, 665, 7, 41, 4488, 125, 370, 1364, 636, 41, 1642, 242, 137, 149, 142, 4009, 190, 59, 9, 2, 11, 41, 1317, 2493, 136, 59, 7565, 35, 436, 1272, 7, 668, 5197, 2, 2, 5, 2269, 6, 543, 446, 42, 142, 43, 1005, 2, 852, 2, 134, 5867, 63, 2453, 1482, 11, 4, 1431, 523, 7, 481, 1307, 21, 466, 49, 1774, 270, 1592, 2057, 12, 1160, 4, 172, 2723, 841, 7, 18, 463, 4, 55, 1967, 49, 539, 81, 63, 385, 46, 187, 4, 172, 58, 5, 63, 82, 2561, 6, 668, 1272, 33, 86, 7334, 4, 1682, 7, 32, 134, 4475, 5186, 633, 2, 56, 11, 6, 3438, 4110, 2066, 602, 21, 103, 1119, 234, 42, 38, 25, 923, 225, 164, 334, 50, 724, 43, 956, 56, 4, 1114, 10, 10, 4, 114, 1160, 51, 186, 40, 6, 1003, 1544, 6, 915, 6812, 11, 3829, 19, 158, 1431, 2512, 29, 4110, 4, 692, 7, 4, 719, 823, 1635, 7071, 2, 12, 56, 17, 35, 3715, 1648, 552, 1019, 37, 3292, 465, 3941, 3959, 11, 2, 6088, 8, 2, 90, 14, 732, 285, 56, 18, 6, 342, 96, 1944, 200, 4, 1544, 4, 823, 1635, 5, 4, 1397, 4, 823, 1635, 494, 3908, 82, 941, 998, 4760, 11, 3062, 2, 279, 198, 31, 7, 4, 3908, 14, 934, 1127, 21, 50, 26, 712, 63, 140, 724, 43, 6, 550, 1062, 50, 26, 111, 665, 63, 100, 28, 343, 6, 176, 7, 2, 31, 324, 7, 35, 6812, 4614, 18, 463, 2677, 23, 4, 3914, 17, 12, 2, 8, 6, 751, 570, 17, 48, 14, 69, 115, 77, 2004, 23, 22, 159, 225, 6, 729, 1483, 8, 6, 801, 7384, 17, 48, 225, 142, 1732, 44, 12, 4, 548, 139, 26, 55, 2, 17, 48, 4, 1057, 69, 8, 361, 4, 86, 304, 8, 763, 49, 365, 1107, 225, 6, 136, 7, 4, 147, 2, 9110, 44, 2, 143, 146, 3099, 50, 71, 998, 352, 712, 262, 3523, 11, 4, 4541, 985, 121, 1214, 481, 302, 5, 3740, 1108, 56, 6, 176, 7, 6514, 206, 141, 17, 4, 583, 7, 60, 147, 267, 1865, 12, 2018, 8, 330, 148, 211, 54, 362, 361, 3161, 1865, 5, 3945, 3589, 26, 112, 3429, 1454, 120, 2, 50, 26, 2442, 7, 2, 141, 17, 8621, 6, 109, 8, 79, 2757, 5, 668, 1006, 35, 402, 2823, 7, 141, 103, 49, 2, 71, 5671, 21, 669, 490, 30, 2660, 129, 523, 632, 342, 1910, 457, 5203, 2, 457, 7904, 470, 1860, 342, 3289, 3227, 342, 2, 470, 7095, 470, 1979, 1086, 1062, 342, 444, 470])\n",
            " list([1, 13, 104, 13, 1176, 4, 4866, 4969, 18, 14, 20, 237, 13, 188, 12, 17, 173, 7, 6, 1647, 1733, 20, 2239, 288, 1594, 19, 1570, 102, 18, 891, 3596, 63, 817, 13, 1539, 142, 40, 1570, 6414, 18, 4, 580, 8, 106, 4, 328, 5552, 310, 7, 6, 20, 15, 16, 626, 17, 2, 10, 10, 18, 6, 1121, 1060, 509, 328, 5552, 218, 99, 78, 60, 151, 12, 9, 540, 2, 34, 6, 364, 352, 31, 7, 61, 2, 2269, 7, 8978, 18, 149, 6, 20, 9, 15, 48, 4, 485, 284, 9, 128, 74, 27, 365, 5, 881, 4, 20, 5347, 214, 33, 222, 289, 381, 15, 9, 434, 4, 420, 133, 2, 47, 49, 1369, 5, 49, 3315, 5, 242, 1816, 6, 128, 22, 611, 74, 29, 188, 10, 10, 4, 890, 548, 3822, 4, 2, 282, 18, 4, 22, 66, 528, 8, 4171, 259, 37, 47, 126, 11, 6, 1647, 1733, 396, 42, 60, 43, 77, 2, 11, 6, 2, 548, 13, 64, 1084, 44, 107, 153, 2799, 1121, 2135, 1879, 21, 60, 13, 62, 115, 809, 18, 4, 1011, 2, 3415, 3131, 3465, 5, 2, 5904, 23, 2444, 133, 21, 4, 841, 9, 52, 4705, 5, 541, 5, 5763, 9909, 5, 4, 156, 276, 49, 547, 83, 4, 548, 139, 10, 10, 329, 867, 9, 4, 114, 2, 109, 2, 9, 1000, 17, 6, 890, 1393, 2455, 185, 132, 37, 271, 267, 18, 157, 17, 6, 1261, 56, 4025, 11, 4667, 890, 4259, 246, 29, 9, 770, 5, 2, 54, 29, 47, 8, 990, 125, 4, 1788, 1338, 72, 21, 13, 16, 2841, 11, 392, 513, 2, 5, 60, 13, 697, 39, 149, 254, 211, 19, 1394, 6182, 15, 4, 1788, 28, 8, 30, 1539, 125, 18, 14, 432, 7, 206, 5, 15, 4, 493, 37, 548, 887, 4, 11, 661, 8, 79, 68, 4259, 5, 15, 4, 71, 290, 4, 278, 38, 25, 28, 8, 106, 14, 20, 19, 6, 432, 7, 2, 5488, 7, 129, 2778, 2, 11, 661, 8, 1779, 12, 17, 6, 328, 597, 112, 6310, 20, 91, 7, 4, 85, 701, 328, 8438, 11, 4, 9707, 79, 2, 11, 4, 393, 18, 4, 172, 855, 99, 422, 10, 10, 4, 20, 2478, 39, 6, 346, 692, 6312, 4, 167, 540, 161, 28, 4, 352, 8, 22, 49, 7, 4, 139, 29, 887, 38, 29, 69, 8, 2233, 11, 4, 7346, 19, 49, 1018, 2761, 5302, 139, 4, 13, 90, 13, 90, 5, 95, 13, 556, 90, 136, 43, 152, 157, 367, 19, 544, 7363, 5, 9474, 15, 26, 4116, 5, 2631, 14, 9, 262, 283, 19, 4, 226, 883, 2651, 63, 186, 8, 28, 77, 814, 17, 48, 12, 71, 35, 2, 14, 9, 6, 117, 5360, 54, 25, 1133, 15, 4, 341, 7, 5078, 341, 9, 424, 8, 7374, 27, 1278, 18, 1060, 10, 10, 21, 280, 4, 20, 8763, 32, 4, 96, 39, 185, 4025, 4683, 143, 4, 3897, 8, 4, 1060, 756, 12, 2865, 56, 6, 117, 6487, 5, 299, 19, 6, 117, 53, 5380, 146, 24, 252, 15, 4, 477, 7447, 9, 290, 4, 2, 2477, 6451, 2018, 14, 432, 7, 155, 6, 196, 1278, 18, 6, 346, 251, 33, 4, 2583, 21, 12, 127, 4228, 183, 125, 11, 6, 3716, 2349, 96, 10, 10, 48, 2, 5811, 69, 93, 14, 22, 19, 6, 147, 352, 5, 4, 172, 177, 2, 732, 5, 1352, 5, 309, 17, 4, 632, 84, 62, 28, 2, 12, 17, 4, 375, 4811, 63, 271, 8, 123, 25, 89, 2, 5, 580, 70, 297, 6656, 19, 62, 30, 2, 1306, 10, 10, 290, 319, 280, 18, 998, 542, 665, 5, 411, 5, 8, 106, 2, 9638, 9343, 309, 11, 6, 217, 15, 9, 4158, 90])] [[1. 0.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClM3xhZffWMC"
      },
      "source": [
        "## Sequence Padding\n",
        "\n",
        "Um problema que temos é que cada sequência de texto tem na maioria dos casos diferentes comprimentos de palavras. Para corrigir isso, vamos usar `pad_sequence()` que simplesmente preenche a sequência de palavras com zeros. O número máximo 250 foi escolhido por uma aproximação da média de palavras na avaliação que é 234. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgdxOi8ja7tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7ea287-290a-4efe-c1f4-f536a163c81a"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(sentences_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(sentences_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[100])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  13  124   12 4283   30    2 3647    7 3384   21   48   40   72   25\n",
            "  119  108   15   26  324 1293    5   28  212  883    5   35  221  114\n",
            "   25   80  119   12   12    9 1061   19   87  105    5 1022    5    2\n",
            "   26  199 1490   38  259   70 6157   33  142 2276 1324  347   25   28\n",
            "    8  202   12    6  353   13  296   12   23  248   31  251   34    2\n",
            "    5  447   12   15   13   69    8  140   46    5 1406   12  180   63\n",
            "  562   49   58    5   13  100  106   12 2905   61  336    4  632   13\n",
            "  119   25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZso-lvrdhHW"
      },
      "source": [
        "##Construindo a Rede Neural\n",
        "\n",
        "Agora podemos construir uma rede neural simples. Começaremos definindo o tipo de modelo que queremos construir. Existem dois tipos de modelos disponíveis no Keras: o modelo sequencial e a classe do modelo usado na API funcional.\n",
        "\n",
        "Começamos adicionando a primeira camada padrão ao lidar com Texto e CNNs: a camada de Embeddings. Nesse exercício vamos usar um Embedding de tamanho 300. Depois, camadas de Convolução e MaxPooling são adicionadas e por fim, camadas de classificação Densas, sendo que a última camada utiliza a função de classificaçao Softmax.\n",
        "Por último, vemos um resumo do modelo que acabamos de construir. OBS: Lembre-se de mudar seu ambiente de execução para GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffrOFeKuL5nR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8024ea-3e8b-4bda-bf9e-e9cc902ffddb"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "vocab_size = X_train.shape[0] + 1 \n",
        "\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 300)          10500300  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 96, 32)            48032     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 10,548,684\n",
            "Trainable params: 10,548,684\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw7A4Zm0gKn8"
      },
      "source": [
        "##Compilação do Modelo\n",
        "\n",
        "Nessa parte do código, definimos os algoritmos de otimização, a função de perda e a métrica que será utilizada para avaliação do modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybvzy_1RgJHc"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtq3pVoMg1qK"
      },
      "source": [
        "##Configuração da Avaliação\n",
        "Reservaremos uma parte de nossos dados de treinamento para validação da precisão do modelo durante o treinamento. Um conjunto de validação nos permite monitorar o progresso de nosso modelo em dados não vistos anteriormente à medida que ele passa por épocas durante o treinamento.\n",
        "As etapas de validação nos ajudam a ajustar os parâmetros de treinamento da função `model.fit` para evitar overfitting e underfitting de dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPKKuXGeMBY_"
      },
      "source": [
        "# Input for Validation\n",
        "X_val = X_train[:5000]\n",
        "partial_X_train = X_train[10000:]\n",
        "\n",
        "# Labels for validation\n",
        "y_val = y_train[:5000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLWSviV6iEWR"
      },
      "source": [
        "##Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BqT9ntbMEPf"
      },
      "source": [
        "history = model.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=512)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeadSTcMUVe"
      },
      "source": [
        "### Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68aYM4fNMU61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3eabdb-744f-429c-89ce-43bbf96fb59e"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "y_true  = np.argmax(y_test, axis=1)\n",
        "preds = (model.predict(X_test) > 0.5).astype(\"int32\")[:, [1]]\n",
        "\n",
        "print(metrics.classification_report(y_true,preds))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9580\n",
            "Testing Accuracy:  0.8570\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86      7500\n",
            "           1       0.86      0.86      0.86      7500\n",
            "\n",
            "    accuracy                           0.86     15000\n",
            "   macro avg       0.86      0.86      0.86     15000\n",
            "weighted avg       0.86      0.86      0.86     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvY8J3x8h31X"
      },
      "source": [
        "##Visualizando os Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR2mgZS7MJ8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "b45df9b2-f6dc-427e-bde3-208d30913989"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "# Plotting losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss_values, 'b', label=\"Validation Loss\")\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEq-noiTOUY5"
      },
      "source": [
        "## Adicionando embeddings pré-treinados\n",
        "É possível usarmos embeddings pré-treinados. A escolha é sempre relativa ao seu problema. Por exemplo, se você precisa resolver um problema de classificação de texto de cunho geral, pode pegar um Embeddings pré-treinado do Google, com milhões de textos. Porém, se quiser resolver um problema de classificação de sentimentos de review de livros, pode ser útil utilizar um embeddings mais próximo do seu problema, como por exemplo, um embeddings pré-treinado com informações e review de livros da Amazon. Nessa atividade, vamos utilizar o Embedding de notícias do Google.\n",
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtI5eg7YOdQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e648e3f-ecc5-4e56-800d-3e3adaca5610"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-16 00:12:11--  https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66209703 (63M) [text/plain]\n",
            "Saving to: ‘googlenews.word2vec.300d.txt’\n",
            "\n",
            "googlenews.word2vec 100%[===================>]  63.14M   161MB/s    in 0.4s    \n",
            "\n",
            "2021-09-16 00:12:16 (161 MB/s) - ‘googlenews.word2vec.300d.txt’ saved [66209703/66209703]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxJbj-nGOiew"
      },
      "source": [
        "### GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGeHCn84OlJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513874aa-05fe-4b04-ba7c-8044c21f5edf"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-16 00:12:17--  https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53933315 (51M) [text/plain]\n",
            "Saving to: ‘glove.840B.300d.sst.txt’\n",
            "\n",
            "glove.840B.300d.sst 100%[===================>]  51.43M   134MB/s    in 0.4s    \n",
            "\n",
            "2021-09-16 00:12:20 (134 MB/s) - ‘glove.840B.300d.sst.txt’ saved [53933315/53933315]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-2wE-W4OxZ_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    \n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "    \n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzn-_iqCO8R4"
      },
      "source": [
        "#Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1p7nyGtO-U3"
      },
      "source": [
        "1. Realize a análise de sentimentos com os Embeddings pré-treinados Word2Vec e Glove, utilizando a rede neural criada em cima. Na tabela abaixo, troque o X pelos resultados da acurácia, do precision, do recall e F1.\n",
        "\n",
        "\\begin{array}{|c|c|c|c|c|}\\hline \n",
        "  Embedding & Precisão & Recall & F1 & Acurácia\\\\  \\hline \n",
        "Word2Vec & 0,88 & 0,88 & 0,88 & 0,88  \\\\ \\hline\n",
        "Glove  & 0,85 & 0,85 & 0,85 & 0,85  \\\\ \\hline \n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWmUlHsyqVA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ff8326-057d-4585-f56e-bb73653fa63d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "\n",
        "#recupera palavras a partir dos indices da base imdb\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {k:v for k,v in word_index.items()}\n",
        "\n",
        "#Cria a matriz de Embeddings\n",
        "embedding_matrix = create_embedding_matrix('glove.840B.300d.sst.txt',\n",
        "                      index_word, embedding_dim)\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "#Cria Rede Neural\n",
        "modelGlove = Sequential()\n",
        "vocab_size = len(index_word) + 1 \n",
        "\n",
        "modelGlove.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           weights=[embedding_matrix],\n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "modelGlove.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "modelGlove.add(layers.GlobalMaxPooling1D())\n",
        "modelGlove.add(layers.Dense(10, activation='relu'))\n",
        "modelGlove.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "modelGlove.summary()\n",
        "\n",
        "#Compila o Modelo \n",
        "modelGlove.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#Treina o Modelo\n",
        "history = modelGlove.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=512)\n",
        "\n",
        "#Avalia o Modelo\n",
        "loss, accuracy = modelGlove.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = modelGlove.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "y_true  = np.argmax(y_test, axis=1)\n",
        "preds = (modelGlove.predict(X_test) > 0.5).astype(\"int32\")[:, [1]]\n",
        "\n",
        "print(metrics.classification_report(y_true,preds))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 300)          26575500  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 96, 32)            48032     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 26,623,884\n",
            "Trainable params: 26,623,884\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training Accuracy: 0.9505\n",
            "Testing Accuracy:  0.8277\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83      7500\n",
            "           1       0.83      0.83      0.83      7500\n",
            "\n",
            "    accuracy                           0.83     15000\n",
            "   macro avg       0.83      0.83      0.83     15000\n",
            "weighted avg       0.83      0.83      0.83     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvLmo-PMPekx"
      },
      "source": [
        "2. Agora, crie uma nova rede neural e altere seus parâmetros para melhorar os resultados obtidos na questão anterior para os Embeddings pré-treinados Word2Vec e Glove. Mantenha o número de épocas em 10. \n",
        "\n",
        "  a) Escreva qual foi sua melhor alteração?\n",
        "      \n",
        "            Após várias tentativas de alterar filters (32,100 e 512), número de camadas convolucionais (até 3), funções de ativação (tanh, linear e relu), dropouts (variando de 0,1 a 0,5 para tentar reduzir o overfiting), ativação do dense final (softmax e sigmoid), otimizadores (Adam, rmsprop e adadelta) e batchsize (50 - 2048) obtive apenas ganhos marginais para os dois embeddings ficando difícil escolher a melhor alteração.\n",
        "\n",
        "\n",
        "  b) Na tabela abaixo, troque o X pelos melhores resultados da acurácia, do precision, do recall e F1.\n",
        "\n",
        "\\begin{array}{|c|c|c|c|c|}\\hline \n",
        "  Embedding & Precisão & Recall & F1 & Acurácia\\\\  \\hline \n",
        "Word2Vec & 0,89 & 0,89 & 0,89 & 0,89  \\\\ \\hline\n",
        "Glove  & 0,89 & 0,89 & 0,89 & 0,89  \\\\ \\hline \n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyM4UzUwp-0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d8dfb0-833b-4a47-e1bd-af43c480b476"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "\n",
        "#recupera palavras a partir dos indices da base imdb\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {k:v for k,v in word_index.items()}\n",
        "\n",
        "#Cria a matriz de Embeddings\n",
        "embedding_matrix = create_embedding_matrix('glove.840B.300d.sst.txt',\n",
        "                      index_word, embedding_dim)\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "#Cria Rede Neural\n",
        "modelGlove = Sequential()\n",
        "vocab_size = len(index_word) + 1 \n",
        "\n",
        "modelGlove.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           weights=[embedding_matrix],\n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "modelGlove.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "modelGlove.add(layers.MaxPool1D(pool_size=2, strides=2))\n",
        "modelGlove.add(layers.Flatten())\n",
        "modelGlove.add(layers.Dropout(0.5))\n",
        "modelGlove.add(layers.Dense(10, activation='relu'))\n",
        "modelGlove.add(layers.Dropout(0.5))\n",
        "modelGlove.add(layers.Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "modelGlove.summary()\n",
        "\n",
        "#Compila o Modelo \n",
        "modelGlove.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#Treina o Modelo\n",
        "history = modelGlove.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=512)\n",
        "\n",
        "#Avalia o Modelo\n",
        "loss, accuracy = modelGlove.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = modelGlove.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "y_true  = np.argmax(y_test, axis=1)\n",
        "preds = (modelGlove.predict(X_test) > 0.5).astype(\"int32\")[:, [1]]\n",
        "\n",
        "print(metrics.classification_report(y_true,preds))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_20 (Embedding)     (None, 300, 300)          26575500  \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 296, 32)           48032     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 148, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 4736)              0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4736)              0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                47370     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 26,670,924\n",
            "Trainable params: 26,670,924\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training Accuracy: 0.9425\n",
            "Testing Accuracy:  0.8907\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.90      0.89      7500\n",
            "           1       0.90      0.88      0.89      7500\n",
            "\n",
            "    accuracy                           0.89     15000\n",
            "   macro avg       0.89      0.89      0.89     15000\n",
            "weighted avg       0.89      0.89      0.89     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLQaup_lARlE",
        "outputId": "0f83dcac-1907-44b3-ae4a-14200e3447d2"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "\n",
        "#recupera palavras a partir dos indices da base imdb\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {k:v for k,v in word_index.items()}\n",
        "\n",
        "#Cria a matriz de Embeddings\n",
        "embedding_matrix = create_embedding_matrix('googlenews.word2vec.300d.txt',\n",
        "                      index_word, embedding_dim)\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "#Cria Rede Neural\n",
        "modelWord2Vec = Sequential()\n",
        "vocab_size = len(index_word) + 1 \n",
        "\n",
        "modelWord2Vec.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           weights=[embedding_matrix],\n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "modelWord2Vec.add(layers.Conv1D(32, 5, padding='same',activation='relu'))\n",
        "modelWord2Vec.add(layers.GlobalMaxPooling1D())\n",
        "modelWord2Vec.add(layers.Flatten())\n",
        "modelWord2Vec.add(layers.Dropout(0.1))\n",
        "modelWord2Vec.add(layers.Dense(10, activation='relu'))\n",
        "modelWord2Vec.add(layers.Dropout(0.1))\n",
        "modelWord2Vec.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "modelWord2Vec.summary()\n",
        "\n",
        "#Compila o Modelo \n",
        "modelWord2Vec.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#Treina o Modelo\n",
        "history = modelWord2Vec.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=512)\n",
        "\n",
        "#Avalia o Modelo\n",
        "loss, accuracy = modelWord2Vec.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = modelWord2Vec.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "y_true  = np.argmax(y_test, axis=1)\n",
        "preds = (modelWord2Vec.predict(X_test) > 0.5).astype(\"int32\")[:, [1]]\n",
        "\n",
        "print(metrics.classification_report(y_true,preds))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 100, 300)          26575500  \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 100, 32)           48032     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 26,623,884\n",
            "Trainable params: 26,623,884\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training Accuracy: 0.9553\n",
            "Testing Accuracy:  0.8482\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85      7500\n",
            "           1       0.85      0.85      0.85      7500\n",
            "\n",
            "    accuracy                           0.85     15000\n",
            "   macro avg       0.85      0.85      0.85     15000\n",
            "weighted avg       0.85      0.85      0.85     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKCzt8CZJv6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8e6918-0354-49e5-d10e-a9b5f1861db2"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "\n",
        "#recupera palavras a partir dos indices da base imdb\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {k:v for k,v in word_index.items()}\n",
        "\n",
        "#Cria a matriz de Embeddings\n",
        "embedding_matrix = create_embedding_matrix('googlenews.word2vec.300d.txt',\n",
        "                      index_word, embedding_dim)\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "#Cria Rede Neural\n",
        "modelWord2Vec = Sequential()\n",
        "vocab_size = len(index_word) + 1 \n",
        "\n",
        "modelWord2Vec.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           weights=[embedding_matrix],\n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "modelWord2Vec.add(layers.Conv1D(32, 5, activation='linear'))\n",
        "modelWord2Vec.add(layers.Conv1D(32, 3, activation='tanh'))\n",
        "modelWord2Vec.add(layers.Conv1D(32, 1, activation='relu'))\n",
        "modelWord2Vec.add(layers.GlobalAveragePooling1D())\n",
        "modelWord2Vec.add(layers.Flatten())\n",
        "modelWord2Vec.add(layers.Dropout(0.1))\n",
        "modelWord2Vec.add(layers.Dense(16, activation='relu'))\n",
        "modelWord2Vec.add(layers.Dropout(0.1))\n",
        "modelWord2Vec.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "modelWord2Vec.summary()\n",
        "\n",
        "#Compila o Modelo \n",
        "modelWord2Vec.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#Treina o Modelo\n",
        "history = modelWord2Vec.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=2042)\n",
        "\n",
        "#Avalia o Modelo\n",
        "loss, accuracy = modelWord2Vec.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = modelWord2Vec.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "y_true  = np.argmax(y_test, axis=1)\n",
        "preds = (modelWord2Vec.predict(X_test) > 0.5).astype(\"int32\")[:, [1]]\n",
        "\n",
        "print(metrics.classification_report(y_true,preds))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_37 (Embedding)     (None, 100, 300)          26575500  \n",
            "_________________________________________________________________\n",
            "conv1d_76 (Conv1D)           (None, 96, 32)            48032     \n",
            "_________________________________________________________________\n",
            "conv1d_77 (Conv1D)           (None, 94, 32)            3104      \n",
            "_________________________________________________________________\n",
            "conv1d_78 (Conv1D)           (None, 94, 32)            1056      \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_14  (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "flatten_28 (Flatten)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 26,628,254\n",
            "Trainable params: 26,628,254\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training Accuracy: 0.9127\n",
            "Testing Accuracy:  0.8337\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.83      7500\n",
            "           1       0.84      0.83      0.83      7500\n",
            "\n",
            "    accuracy                           0.83     15000\n",
            "   macro avg       0.83      0.83      0.83     15000\n",
            "weighted avg       0.83      0.83      0.83     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiWyufHcAE3p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}